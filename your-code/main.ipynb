{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Web Scraping Lab\n",
    "\n",
    "You will find in this notebook some scrapy exercises to practise your scraping skills.\n",
    "\n",
    "**Tips:**\n",
    "\n",
    "- Check the response status code for each request to ensure you have obtained the intended contennt.\n",
    "- Print the response text in each request to understand the kind of info you are getting and its format.\n",
    "- Check for patterns in the response text to extract the data/info requested in each question.\n",
    "- Visit each url and take a look at its source through Chrome DevTools. You'll need to identify the html tags, special class names etc. used for the html content you are expected to extract."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- [Requests library](http://docs.python-requests.org/en/master/#the-user-guide) documentation \n",
    "- [Beautiful Soup Doc](https://www.crummy.com/software/BeautifulSoup/bs4/doc/)\n",
    "- [Urllib](https://docs.python.org/3/library/urllib.html#module-urllib)\n",
    "- [re lib](https://docs.python.org/3/library/re.html)\n",
    "- [lxml lib](https://lxml.de/)\n",
    "- [Scrapy](https://scrapy.org/)\n",
    "- [List of HTTP status codes](https://en.wikipedia.org/wiki/List_of_HTTP_status_codes)\n",
    "- [HTML basics](http://www.simplehtmlguide.com/cheatsheet.php)\n",
    "- [CSS basics](https://www.cssbasics.com/#page_start)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Below are the libraries and modules you may need. `requests`,  `BeautifulSoup` and `pandas` are imported for you. If you prefer to use additional libraries feel free to uncomment them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "# from pprint import pprint\n",
    "# from lxml import html\n",
    "# from lxml.html import fromstring\n",
    "# import urllib.request\n",
    "# from urllib.request import urlopen\n",
    "# import random\n",
    "# import re\n",
    "# import scrapy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Download, parse (using BeautifulSoup), and print the content from the Trending Developers page from GitHub:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the url you will scrape in this exercise\n",
    "url = 'https://github.com/trending/developers'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "#your code\n",
    "response = requests.get(url)\n",
    "\n",
    "soup = BeautifulSoup(response.text, 'html.parser')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Display the names of the trending developers retrieved in the previous step.\n",
    "\n",
    "Your output should be a Python list of developer names. Each name should not contain any html tag.\n",
    "\n",
    "**Instructions:**\n",
    "\n",
    "1. Find out the html tag and class names used for the developer names. You can achieve this using Chrome DevTools.\n",
    "\n",
    "1. Use BeautifulSoup to extract all the html elements that contain the developer names.\n",
    "\n",
    "1. Use string manipulation techniques to replace whitespaces and linebreaks (i.e. `\\n`) in the *text* of each html element. Use a list to store the clean names.\n",
    "\n",
    "1. Print the list of names.\n",
    "\n",
    "Your output should look like below:\n",
    "\n",
    "```\n",
    "['trimstray (@trimstray)',\n",
    " 'joewalnes (JoeWalnes)',\n",
    " 'charlax (Charles-AxelDein)',\n",
    " 'ForrestKnight (ForrestKnight)',\n",
    " 'revery-ui (revery-ui)',\n",
    " 'alibaba (Alibaba)',\n",
    " 'Microsoft (Microsoft)',\n",
    " 'github (GitHub)',\n",
    " 'facebook (Facebook)',\n",
    " 'boazsegev (Bo)',\n",
    " 'google (Google)',\n",
    " 'cloudfetch',\n",
    " 'sindresorhus (SindreSorhus)',\n",
    " 'tensorflow',\n",
    " 'apache (TheApacheSoftwareFoundation)',\n",
    " 'DevonCrawford (DevonCrawford)',\n",
    " 'ARMmbed (ArmMbed)',\n",
    " 'vuejs (vuejs)',\n",
    " 'fastai (fast.ai)',\n",
    " 'QiShaoXuan (Qi)',\n",
    " 'joelparkerhenderson (JoelParkerHenderson)',\n",
    " 'torvalds (LinusTorvalds)',\n",
    " 'CyC2018',\n",
    " 'komeiji-satori (神楽坂覚々)',\n",
    " 'script-8']\n",
    " ```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Francois Zaninotto\n",
      "Fons van der Plas\n",
      "Rich Harris\n",
      "Stefano Gottardo\n",
      "Jesse Duffield\n",
      "Ha Thach\n",
      "francisco souza\n",
      "Barry vd. Heuvel\n",
      "Jonathan Reinink\n",
      "Brad Fitzpatrick\n",
      "Sebastián Ramírez\n",
      "David Tolnay\n",
      "Mladen Macanović\n",
      "Gleb Bahmutov\n",
      "Artem Zakharchenko\n",
      "Mike McQuaid\n",
      "Omry Yadan\n",
      "Mariusz Nowak\n",
      "Eric Liu\n",
      "Mike Penz\n",
      "Steve Smith\n",
      "Marten Seemann\n",
      "Josh Bleecher Snyder\n",
      "Jacob Quinn\n",
      "Hadley Wickham\n"
     ]
    }
   ],
   "source": [
    "#your code\n",
    "for developer in soup.find_all(name='h1', attrs={'class':'h3 lh-condensed'}):\n",
    "    print(developer.find(name='a').text.strip())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Display the trending Python repositories in GitHub\n",
    "\n",
    "The steps to solve this problem is similar to the previous one except that you need to find out the repository names instead of developer names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# This is the url you will scrape in this exercise\n",
    "url = 'https://github.com/trending/python?since=daily'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Faker\n",
      "Pluto.jl\n",
      "degit\n",
      "plugin.video.netflix\n",
      "lazygit\n",
      "tinyusb\n",
      "fake-gcs-server\n",
      "laravel-debugbar\n",
      "gomemcache\n",
      "fastapi\n",
      "cxx\n",
      "Blazorise\n",
      "npm-install\n",
      "naming-cheatsheet\n",
      "strap\n",
      "omegaconf\n",
      "memoizee\n",
      "svelte-typeahead\n",
      "release-changelog-builder-action\n",
      "CleanArchitecture\n",
      "quic-network-simulator\n",
      "impl\n",
      "JSON3.jl\n",
      "r4ds\n"
     ]
    }
   ],
   "source": [
    "#your code\n",
    "\n",
    "for developer in soup.find_all(name='h1', attrs={'class':'h4 lh-condensed'}):\n",
    "    print(developer.find(name='a').text.strip())\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Display all the image links from Walt Disney wikipedia page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the url you will scrape in this exercise\n",
    "url = 'https://en.wikipedia.org/wiki/Walt_Disney'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#your code\n",
    "response = requests.get(url)\n",
    "soup = BeautifulSoup(response.text, 'html.parser')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/wiki/File:Walt_Disney_1946.JPG\n",
      "/wiki/File:Walt_Disney_1942_signature.svg\n",
      "/wiki/File:Walt_Disney_envelope_ca._1921.jpg\n",
      "/wiki/File:Trolley_Troubles_poster.jpg\n",
      "/wiki/File:Walt_Disney_and_his_cartoon_creation_%22Mickey_Mouse%22_-_National_Board_of_Review_Magazine.jpg\n",
      "/wiki/File:Steamboat-willie.jpg\n",
      "/wiki/File:Walt_Disney_1935.jpg\n",
      "/wiki/File:Walt_Disney_Snow_white_1937_trailer_screenshot_(13).jpg\n",
      "/wiki/File:Disney_drawing_goofy.jpg\n",
      "/wiki/File:DisneySchiphol1951.jpg\n",
      "/wiki/File:WaltDisneyplansDisneylandDec1954.jpg\n",
      "/wiki/File:Walt_disney_portrait_right.jpg\n",
      "/wiki/File:Walt_Disney_Grave.JPG\n",
      "/wiki/File:Roy_O._Disney_with_Company_at_Press_Conference.jpg\n",
      "/wiki/File:Disney_Display_Case.JPG\n",
      "/wiki/File:Disney1968.jpg\n",
      "/wiki/File:Disneyland_Resort_logo.svg\n",
      "/wiki/File:Animation_disc.svg\n",
      "/wiki/File:P_vip.svg\n",
      "/wiki/File:Magic_Kingdom_castle.jpg\n",
      "/wiki/File:Video-x-generic.svg\n",
      "/wiki/File:Flag_of_Los_Angeles_County,_California.svg\n",
      "/wiki/File:Blank_television_set.svg\n",
      "/wiki/File:Flag_of_the_United_States.svg\n"
     ]
    }
   ],
   "source": [
    "for developer in soup.find_all(name='a', attrs={'class':'image'}):\n",
    "    print(developer['href'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Retrieve an arbitary Wikipedia page of \"Python\" and create a list of links on that page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the url you will scrape in this exercise\n",
    "url ='https://en.wikipedia.org/wiki/Python' "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "#your code\n",
    "from random import choice\n",
    "\n",
    "response = requests.get(url) #Sacamos el html de la pagina de Python\n",
    "soup = BeautifulSoup(response.text, 'html.parser') #Lo convertimos en sopa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/wiki/Python_(Efteling)'"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pythonlinks = [] #Sacamos los links del cuerpo que nos interesan\n",
    "\n",
    "for element in soup.find(name='div', attrs={'class':'mw-parser-output'}).find_all(name='li', attrs={'class':''}):   \n",
    "    pythonlinks.append(element.find(name='a')['href']) #Se crea una lista con estos links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://en.wikipedia.org' + choice(pythonlinks) #Se elige de toda la lista uno al azar y montamos una url\n",
    "\n",
    "#Se repite el proceso.\n",
    "response = requests.get(url) #Sacamos el html de la pagina.\n",
    "soup = BeautifulSoup(response.text, 'html.parser') #Lo convertimos en sopa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "listoflinks=[]\n",
    "for element in soup.find_all(name='a', href=True):\n",
    "    listoflinks.append(element['href'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Number of Titles that have changed in the United States Code since its last release point "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the url you will scrape in this exercise\n",
    "#Titles in bold have been changed since the last release point.\n",
    "\n",
    "url = 'http://uscode.house.gov/download/download.shtml'\n",
    "response = requests.get(url) #Sacamos el html de la pagina de Python\n",
    "soup = BeautifulSoup(response.text, 'html.parser') #Lo convertimos en sopa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n"
     ]
    }
   ],
   "source": [
    "bolded_titles = [element.get_text().replace('٭','').strip() for element in soup.find_all(name='div', attrs={'class':\"usctitlechanged\"})]\n",
    "\n",
    "print(len(bolded_titles))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### A Python list with the top ten FBI's Most Wanted names "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the url you will scrape in this exercise\n",
    "url = 'https://www.fbi.gov/wanted/topten'\n",
    "response = requests.get(url) #Sacamos el html de la pagina de Python\n",
    "soup = BeautifulSoup(response.text, 'html.parser') #Lo convertimos en sopa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ALEJANDRO ROSALES CASTILLO', 'ARNOLDO JIMENEZ', 'JASON DEREK BROWN', 'ALEXIS FLORES', 'JOSE RODOLFO VILLARREAL-HERNANDEZ', 'EUGENE PALMER', 'RAFAEL CARO-QUINTERO', 'ROBERT WILLIAM FISHER', 'BHADRESHKUMAR CHETANBHAI PATEL', 'YASER ABDEL SAID']\n"
     ]
    }
   ],
   "source": [
    "#your code \n",
    "toptensoup = soup.find(name='ul', attrs={'class':\"full-grid wanted-grid-natural infinity castle-grid-block-xs-2 castle-grid-block-sm-2castle-grid-block-md-3 castle-grid-block-lg-5 dt-grid\"})\n",
    "\n",
    "toptencriminals=[]\n",
    "for element in toptensoup.find_all(name='h3', attrs={'class':\"title\"}):\n",
    "    toptencriminals.append(element.text.strip())\n",
    "    \n",
    "print(toptencriminals)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  20 latest earthquakes info (date, time, latitude, longitude and region name) by the EMSC as a pandas dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the url you will scrape in this exercise\n",
    "url = 'https://www.emsc-csem.org/Earthquake/'\n",
    "response = requests.get(url) #Sacamos el html de la pagina de Python\n",
    "soup = BeautifulSoup(response.text, 'html.parser') #Lo convertimos en sopa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "#your code\n",
    "soupbody = soup.find_all(name='tr', attrs={'class':[\"ligne1\",\"ligne2\"]} ) #Se coge cada fila de elementos\n",
    "\n",
    "dic_earthquakes = {'DateTime':[element.find(name='b').find(name='a').text for element in soupbody],\n",
    "                  'Latitude': [element.find_all(name='td',attrs={'class':\"tabev1\"})[0].text + element.find_all(name='td',attrs={'class':\"tabev2\"})[0].text for element in soupbody],\n",
    "                  'Longitude': [element.find_all(name='td',attrs={'class':\"tabev1\"})[1].text + element.find_all(name='td',attrs={'class':\"tabev2\"})[1].text for element in soupbody],\n",
    "                  'Region name': [element.find(name='td',attrs={'class':\"tb_region\"}).text for element in soupbody]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>DateTime</th>\n",
       "      <th>Latitude</th>\n",
       "      <th>Longitude</th>\n",
       "      <th>Region name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2021-04-05 17:47:48.000</td>\n",
       "      <td>19.17 N</td>\n",
       "      <td>155.46 W</td>\n",
       "      <td>ISLAND OF HAWAII, HAWAII</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2021-04-05 17:42:48.600</td>\n",
       "      <td>58.50 N</td>\n",
       "      <td>137.06 W</td>\n",
       "      <td>SOUTHEASTERN ALASKA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2021-04-05 17:31:16.700</td>\n",
       "      <td>47.47 S</td>\n",
       "      <td>166.25 E</td>\n",
       "      <td>OFF W. COAST OF S. ISLAND, N.Z.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2021-04-05 17:29:48.000</td>\n",
       "      <td>19.86 N</td>\n",
       "      <td>155.51 W</td>\n",
       "      <td>ISLAND OF HAWAII, HAWAII</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2021-04-05 16:59:42.300</td>\n",
       "      <td>38.00 N</td>\n",
       "      <td>14.70 E</td>\n",
       "      <td>SICILY, ITALY</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2021-04-05 16:49:41.700</td>\n",
       "      <td>19.25 N</td>\n",
       "      <td>155.42 W</td>\n",
       "      <td>ISLAND OF HAWAII, HAWAII</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2021-04-05 16:43:31.200</td>\n",
       "      <td>58.38 N</td>\n",
       "      <td>137.18 W</td>\n",
       "      <td>SOUTHEASTERN ALASKA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2021-04-05 16:42:31.600</td>\n",
       "      <td>18.60 N</td>\n",
       "      <td>68.91 W</td>\n",
       "      <td>DOMINICAN REPUBLIC</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2021-04-05 16:42:01.500</td>\n",
       "      <td>38.80 N</td>\n",
       "      <td>122.73 W</td>\n",
       "      <td>NORTHERN CALIFORNIA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>2021-04-05 16:41:58.000</td>\n",
       "      <td>3.59 S</td>\n",
       "      <td>80.82 W</td>\n",
       "      <td>PERU-ECUADOR BORDER REGION</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>2021-04-05 16:41:43.700</td>\n",
       "      <td>58.43 N</td>\n",
       "      <td>137.06 W</td>\n",
       "      <td>SOUTHEASTERN ALASKA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>2021-04-05 16:29:19.300</td>\n",
       "      <td>38.01 N</td>\n",
       "      <td>14.70 E</td>\n",
       "      <td>SICILY, ITALY</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>2021-04-05 16:15:55.000</td>\n",
       "      <td>37.87 N</td>\n",
       "      <td>27.62 E</td>\n",
       "      <td>WESTERN TURKEY</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>2021-04-05 16:08:48.500</td>\n",
       "      <td>33.46 N</td>\n",
       "      <td>116.57 W</td>\n",
       "      <td>SOUTHERN CALIFORNIA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>2021-04-05 15:36:06.800</td>\n",
       "      <td>58.39 N</td>\n",
       "      <td>137.16 W</td>\n",
       "      <td>SOUTHEASTERN ALASKA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>2021-04-05 15:20:25.600</td>\n",
       "      <td>37.74 S</td>\n",
       "      <td>179.62 E</td>\n",
       "      <td>OFF E. COAST OF N. ISLAND, N.Z.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>2021-04-05 15:20:00.600</td>\n",
       "      <td>27.17 N</td>\n",
       "      <td>88.88 E</td>\n",
       "      <td>SIKKIM, INDIA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>2021-04-05 15:05:02.700</td>\n",
       "      <td>36.45 N</td>\n",
       "      <td>27.16 E</td>\n",
       "      <td>DODECANESE IS.-TURKEY BORDER REG</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>2021-04-05 14:57:54.400</td>\n",
       "      <td>33.93 N</td>\n",
       "      <td>118.34 W</td>\n",
       "      <td>GREATER LOS ANGELES AREA, CALIF.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>2021-04-05 14:54:07.000</td>\n",
       "      <td>12.91 N</td>\n",
       "      <td>86.07 W</td>\n",
       "      <td>NICARAGUA</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  DateTime   Latitude   Longitude  \\\n",
       "0  2021-04-05 17:47:48.000  19.17 N    155.46 W     \n",
       "1  2021-04-05 17:42:48.600  58.50 N    137.06 W     \n",
       "2  2021-04-05 17:31:16.700  47.47 S    166.25 E     \n",
       "3  2021-04-05 17:29:48.000  19.86 N    155.51 W     \n",
       "4  2021-04-05 16:59:42.300  38.00 N     14.70 E     \n",
       "5  2021-04-05 16:49:41.700  19.25 N    155.42 W     \n",
       "6  2021-04-05 16:43:31.200  58.38 N    137.18 W     \n",
       "7  2021-04-05 16:42:31.600  18.60 N     68.91 W     \n",
       "8  2021-04-05 16:42:01.500  38.80 N    122.73 W     \n",
       "9  2021-04-05 16:41:58.000   3.59 S     80.82 W     \n",
       "10 2021-04-05 16:41:43.700  58.43 N    137.06 W     \n",
       "11 2021-04-05 16:29:19.300  38.01 N     14.70 E     \n",
       "12 2021-04-05 16:15:55.000  37.87 N     27.62 E     \n",
       "13 2021-04-05 16:08:48.500  33.46 N    116.57 W     \n",
       "14 2021-04-05 15:36:06.800  58.39 N    137.16 W     \n",
       "15 2021-04-05 15:20:25.600  37.74 S    179.62 E     \n",
       "16 2021-04-05 15:20:00.600  27.17 N     88.88 E     \n",
       "17 2021-04-05 15:05:02.700  36.45 N     27.16 E     \n",
       "18 2021-04-05 14:57:54.400  33.93 N    118.34 W     \n",
       "19 2021-04-05 14:54:07.000  12.91 N     86.07 W     \n",
       "\n",
       "                          Region name  \n",
       "0            ISLAND OF HAWAII, HAWAII  \n",
       "1                 SOUTHEASTERN ALASKA  \n",
       "2     OFF W. COAST OF S. ISLAND, N.Z.  \n",
       "3            ISLAND OF HAWAII, HAWAII  \n",
       "4                       SICILY, ITALY  \n",
       "5            ISLAND OF HAWAII, HAWAII  \n",
       "6                 SOUTHEASTERN ALASKA  \n",
       "7                  DOMINICAN REPUBLIC  \n",
       "8                 NORTHERN CALIFORNIA  \n",
       "9          PERU-ECUADOR BORDER REGION  \n",
       "10                SOUTHEASTERN ALASKA  \n",
       "11                      SICILY, ITALY  \n",
       "12                     WESTERN TURKEY  \n",
       "13                SOUTHERN CALIFORNIA  \n",
       "14                SOUTHEASTERN ALASKA  \n",
       "15    OFF E. COAST OF N. ISLAND, N.Z.  \n",
       "16                      SIKKIM, INDIA  \n",
       "17   DODECANESE IS.-TURKEY BORDER REG  \n",
       "18   GREATER LOS ANGELES AREA, CALIF.  \n",
       "19                          NICARAGUA  "
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame(dic_earthquakes)\n",
    "df['DateTime'] = pd.to_datetime(df['DateTime'])   \n",
    "df.sort_values('DateTime',ascending=False).head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Count number of tweets by a given Twitter account."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You will need to include a ***try/except block*** for account names not found. \n",
    "<br>***Hint:*** the program should count the number of tweets for any provided account"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# This is the url you will scrape in this exercise \n",
    "# You will need to add the account credentials to this url\n",
    "\n",
    "#No se puede scrapear twitter con beautiful soup, volver con Selenium o la API de twitter\n",
    "\n",
    "url = 'https://twitter.com/albert_rivera'\n",
    "url = 'https://twitter.com/perezreverte'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Number of followers of a given twitter account"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You will need to include a ***try/except block*** in case account/s name not found. \n",
    "<br>***Hint:*** the program should count the followers for any provided account"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the url you will scrape in this exercise \n",
    "# You will need to add the account credentials to this url\n",
    "url = 'https://twitter.com/albert_rivera'\n",
    "url = 'https://twitter.com/perezreverte'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [],
   "source": [
    "#your code\n",
    "headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/89.0.4389.114 Safari/537.36'}\n",
    "response = requests.get(url, headers=headers) #Sacamos el html de la pagina de Python\n",
    "soup = BeautifulSoup(response.text, 'html.parser') #Lo convertimos en sopa"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### List all language names and number of related articles in the order they appear in wikipedia.org"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 325,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the url you will scrape in this exercise\n",
    "url = 'https://en.wikipedia.org/wiki/Battle_of_Fallen_Timbers'\n",
    "response = requests.get(url, headers=headers) #Sacamos el html de la pagina de Python\n",
    "soup = BeautifulSoup(response.text, 'html.parser') #Lo convertimos en sopa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 328,
   "metadata": {},
   "outputs": [],
   "source": [
    "#your code\n",
    "list_of_language = soup.find(name='nav', attrs={'id': 'p-lang'}).find_all(name='a', attrs={'class': 'interlanguage-link-target'})\n",
    "\n",
    "[str(language['title']) for language in list_of_language]\n",
    "\n",
    "ls=[]\n",
    "for language in list_of_language:\n",
    "   ls.append((language['title']))\n",
    "\n",
    "#Related articles?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### A list with the different kind of datasets available in data.gov.uk "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the url you will scrape in this exercise\n",
    "url = 'https://data.gov.uk/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Business and economy', 'Small businesses, industry, imports, exports and trade'), ('Crime and justice', 'Courts, police, prison, offenders, borders and immigration'), ('Defence', 'Armed forces, health and safety, search and rescue'), ('Education', 'Students, training, qualifications and the National Curriculum'), ('Environment', 'Weather, flooding, rivers, air quality, geology and agriculture'), ('Government', 'Staff numbers and pay, local councillors and department business plans'), ('Government spending', 'Includes all payments by government departments over £25,000'), ('Health', 'Includes smoking, drugs, alcohol, medicine performance and hospitals'), ('Mapping', 'Addresses, boundaries, land ownership, aerial photographs, seabed and land terrain'), ('Society', 'Employment, benefits, household finances, poverty and population'), ('Towns and cities', 'Includes housing, urban planning, leisure, waste and energy, consumption'), ('Transport', 'Airports, roads, freight, electric vehicles, parking, buses and footpaths'), ('Digital service performance', 'Cost, usage, completion rate, digital take-up, satisfaction'), ('Government reference data', 'Trusted data that is referenced and shared across government departments')]\n"
     ]
    }
   ],
   "source": [
    "#your code \n",
    "headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/89.0.4389.114 Safari/537.36'}\n",
    "# This is the url you will scrape in this exercise\n",
    "response = requests.get(url, headers=headers) #Sacamos el html de la pagina de Python\n",
    "soup = BeautifulSoup(response.text, 'html.parser') #Lo convertimos en sopa\n",
    "\n",
    "\n",
    "DataSets = soup.find(name='ul', attrs={'class':\"govuk-list\"}).find_all(name= 'a', attrs={'class':\"govuk-link\"})\n",
    "Theme = soup.find(name='ul', attrs={'class':\"govuk-list\"}).find_all(name= 'p', attrs={'class':\"govuk-body\"})\n",
    "\n",
    "DataSets = [element.text for element in DataSets]\n",
    "Theme = [element.text for element in Theme]\n",
    "\n",
    "print(list(zip(DataSets,Theme)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Top 10 languages by number of native speakers stored in a Pandas Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 330,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the url you will scrape in this exercise\n",
    "url = 'https://en.wikipedia.org/wiki/List_of_languages_by_number_of_native_speakers'\n",
    "\n",
    "response = requests.get(url, headers=headers) #Sacamos el html de la pagina de Python\n",
    "soup = BeautifulSoup(response.text, 'html.parser') #Lo convertimos en sopa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 370,
   "metadata": {},
   "outputs": [],
   "source": [
    "#your code\n",
    "nativespeakertable = soup.find_all(name='table',attrs={'class': 'wikitable' })[1]\n",
    "\n",
    "\n",
    "\n",
    "dic_earthquakes = {'Language':[languages.contents[3].text for languages in nativespeakertable.find_all(name='tr')[1:]],\n",
    "                    'Native Speakers': [languages.contents[5].text for languages in nativespeakertable.find_all(name='tr')[1:]]                  }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 372,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Language</th>\n",
       "      <th>Native Speakers</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Mandarin (entire branch)</td>\n",
       "      <td>935 (955)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Spanish</td>\n",
       "      <td>390 (405)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>English</td>\n",
       "      <td>365 (360)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Hindi[a]</td>\n",
       "      <td>295 (310)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Arabic</td>\n",
       "      <td>280 (295)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Portuguese</td>\n",
       "      <td>205 (215)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Bengali</td>\n",
       "      <td>200 (205)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Russian</td>\n",
       "      <td>160 (155)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Japanese</td>\n",
       "      <td>125 (125)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Punjabi</td>\n",
       "      <td>95 (100)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   Language Native Speakers\n",
       "0  Mandarin (entire branch)       935 (955)\n",
       "1                   Spanish       390 (405)\n",
       "2                   English       365 (360)\n",
       "3                  Hindi[a]       295 (310)\n",
       "4                    Arabic       280 (295)\n",
       "5                Portuguese       205 (215)\n",
       "6                   Bengali       200 (205)\n",
       "7                   Russian       160 (155)\n",
       "8                  Japanese       125 (125)\n",
       "9                   Punjabi        95 (100)"
      ]
     },
     "execution_count": 372,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame(dic_earthquakes)\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BONUS QUESTIONS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Scrape a certain number of tweets of a given Twitter account."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# This is the url you will scrape in this exercise \n",
    "# You will need to add the account credentials to this url\n",
    "url = 'https://twitter.com/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# your code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### IMDB's Top 250 data (movie name, Initial release, director name and stars) as a pandas dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the url you will scrape in this exercise \n",
    "url = 'https://www.imdb.com/chart/top'\n",
    "\n",
    "response = requests.get(url, headers=headers) #Sacamos el html de la pagina de Python\n",
    "soup = BeautifulSoup(response.text, 'html.parser') #Lo convertimos en sopa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Titulo</th>\n",
       "      <th>Director</th>\n",
       "      <th>Stars</th>\n",
       "      <th>ReleaseDate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Cadena perpetua</td>\n",
       "      <td>Frank Darabont</td>\n",
       "      <td>9.2</td>\n",
       "      <td>1994</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>El padrino</td>\n",
       "      <td>Francis Ford Coppola</td>\n",
       "      <td>9.1</td>\n",
       "      <td>1972</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>El padrino: Parte II</td>\n",
       "      <td>Francis Ford Coppola</td>\n",
       "      <td>9.0</td>\n",
       "      <td>1974</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>El caballero oscuro</td>\n",
       "      <td>Christopher Nolan</td>\n",
       "      <td>9.0</td>\n",
       "      <td>2008</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>12 hombres sin piedad</td>\n",
       "      <td>Sidney Lumet</td>\n",
       "      <td>8.9</td>\n",
       "      <td>1957</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>La lista de Schindler</td>\n",
       "      <td>Steven Spielberg</td>\n",
       "      <td>8.9</td>\n",
       "      <td>1993</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>El señor de los anillos: El retorno del rey</td>\n",
       "      <td>Peter Jackson</td>\n",
       "      <td>8.9</td>\n",
       "      <td>2003</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Pulp Fiction</td>\n",
       "      <td>Quentin Tarantino</td>\n",
       "      <td>8.8</td>\n",
       "      <td>1994</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>El bueno, el feo y el malo</td>\n",
       "      <td>Sergio Leone</td>\n",
       "      <td>8.8</td>\n",
       "      <td>1966</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>El señor de los anillos: La comunidad del anillo</td>\n",
       "      <td>Peter Jackson</td>\n",
       "      <td>8.8</td>\n",
       "      <td>2001</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             Titulo              Director  \\\n",
       "0                                   Cadena perpetua        Frank Darabont   \n",
       "1                                        El padrino  Francis Ford Coppola   \n",
       "2                              El padrino: Parte II  Francis Ford Coppola   \n",
       "3                               El caballero oscuro     Christopher Nolan   \n",
       "4                             12 hombres sin piedad          Sidney Lumet   \n",
       "5                             La lista de Schindler      Steven Spielberg   \n",
       "6       El señor de los anillos: El retorno del rey         Peter Jackson   \n",
       "7                                      Pulp Fiction     Quentin Tarantino   \n",
       "8                        El bueno, el feo y el malo          Sergio Leone   \n",
       "9  El señor de los anillos: La comunidad del anillo         Peter Jackson   \n",
       "\n",
       "   Stars  ReleaseDate  \n",
       "0    9.2         1994  \n",
       "1    9.1         1972  \n",
       "2    9.0         1974  \n",
       "3    9.0         2008  \n",
       "4    8.9         1957  \n",
       "5    8.9         1993  \n",
       "6    8.9         2003  \n",
       "7    8.8         1994  \n",
       "8    8.8         1966  \n",
       "9    8.8         2001  "
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# your code\n",
    "soupbody = soup.find(name='tbody', attrs={'class':'lister-list'} ).find_all(name='tr') #Se coge toda la tabla\n",
    "\n",
    "soupbody[0].find(name='span', attrs={'class':\"secondaryInfo\"} ).get_text()\n",
    "\n",
    "#Titulo = [element.find_all(name='a')[1].get_text() for element in soupbody]\n",
    "#Director = [element.find_all(name='a')[1]['title'].split(',')[0].replace(' (dir.)','') for element in soupbody]\n",
    "#Stars = [element.find(name='strong').get_text() for element in soupbody]\n",
    "#ReleaseDate = [int(element.find(name='span', attrs={'class':\"secondaryInfo\"} ).get_text()[1:-1]) for element in soupbody]\n",
    "\n",
    "dic_top200 = {'Titulo': [element.find_all(name='a')[1].get_text() for element in soupbody],\n",
    "                  'Director': [element.find_all(name='a')[1]['title'].split(',')[0].replace(' (dir.)','') for element in soupbody],\n",
    "                  'Stars':  [float(element.find(name='strong').get_text()) for element in soupbody],\n",
    "                  'ReleaseDate': [int(element.find(name='span', attrs={'class':\"secondaryInfo\"} ).get_text()[1:-1]) for element in soupbody]}\n",
    "\n",
    "df = pd.DataFrame(dic_top200)\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Movie name, year and a brief summary of the top 10 random movies (IMDB) as a pandas dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This is the url you will scrape in this exercise\n",
    "url = 'http://www.imdb.com/chart/top'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "#your code\n",
    "url = 'https://www.imdb.com/chart/top'\n",
    "\n",
    "response = requests.get(url, headers=headers) #Sacamos el html de la pagina de Python\n",
    "soup = BeautifulSoup(response.text, 'html.parser') #Lo convertimos en sopa\n",
    "\n",
    "soupbody = soup.find(name='tbody', attrs={'class':'lister-list'} ).find_all(name='tr') #Se coge toda la tabla\n",
    "\n",
    "soupbody[0].find(name='span', attrs={'class':\"secondaryInfo\"} ).get_text()\n",
    "\n",
    "Link = [element.find_all(name='a')[1]['href'] for element in soupbody][:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary = []\n",
    "for movies in range(0,10):\n",
    "    url = 'https://www.imdb.com/'+ Link[movies]\n",
    "    response = requests.get(url, headers=headers) #Sacamos el html de la pagina de Python\n",
    "    soup = BeautifulSoup(response.text, 'html.parser') #Lo convertimos en sopa\n",
    "    summary.append(soup.find(name='div', attrs={'class':'summary_text'} ).get_text().strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Titulo</th>\n",
       "      <th>Director</th>\n",
       "      <th>Stars</th>\n",
       "      <th>ReleaseDate</th>\n",
       "      <th>Summary</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Cadena perpetua</td>\n",
       "      <td>Frank Darabont</td>\n",
       "      <td>9.2</td>\n",
       "      <td>1994</td>\n",
       "      <td>Two imprisoned men bond over a number of years...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>El padrino</td>\n",
       "      <td>Francis Ford Coppola</td>\n",
       "      <td>9.1</td>\n",
       "      <td>1972</td>\n",
       "      <td>An organized crime dynasty's aging patriarch t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>El padrino: Parte II</td>\n",
       "      <td>Francis Ford Coppola</td>\n",
       "      <td>9.0</td>\n",
       "      <td>1974</td>\n",
       "      <td>The early life and career of Vito Corleone in ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>El caballero oscuro</td>\n",
       "      <td>Christopher Nolan</td>\n",
       "      <td>9.0</td>\n",
       "      <td>2008</td>\n",
       "      <td>When the menace known as the Joker wreaks havo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>12 hombres sin piedad</td>\n",
       "      <td>Sidney Lumet</td>\n",
       "      <td>8.9</td>\n",
       "      <td>1957</td>\n",
       "      <td>A jury holdout attempts to prevent a miscarria...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  Titulo              Director  Stars  ReleaseDate  \\\n",
       "0        Cadena perpetua        Frank Darabont    9.2         1994   \n",
       "1             El padrino  Francis Ford Coppola    9.1         1972   \n",
       "2   El padrino: Parte II  Francis Ford Coppola    9.0         1974   \n",
       "3    El caballero oscuro     Christopher Nolan    9.0         2008   \n",
       "4  12 hombres sin piedad          Sidney Lumet    8.9         1957   \n",
       "\n",
       "                                             Summary  \n",
       "0  Two imprisoned men bond over a number of years...  \n",
       "1  An organized crime dynasty's aging patriarch t...  \n",
       "2  The early life and career of Vito Corleone in ...  \n",
       "3  When the menace known as the Joker wreaks havo...  \n",
       "4  A jury holdout attempts to prevent a miscarria...  "
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dic_top10 = {'Titulo': [element.find_all(name='a')[1].get_text() for element in soupbody][:10],\n",
    "                  'Director': [element.find_all(name='a')[1]['title'].split(',')[0].replace(' (dir.)','') for element in soupbody][:10],\n",
    "                  'Stars':  [float(element.find(name='strong').get_text()) for element in soupbody][:10],\n",
    "                  'ReleaseDate': [int(element.find(name='span', attrs={'class':\"secondaryInfo\"} ).get_text()[1:-1]) for element in soupbody][:10],\n",
    "                  'Summary' : summary[:10]}\n",
    "\n",
    "df = pd.DataFrame(dic_top10)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Find the live weather report (temperature, wind speed, description and weather) of a given city."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#https://openweathermap.org/current\n",
    "city = city=input('Enter the city:')\n",
    "url = 'http://api.openweathermap.org/data/2.5/weather?'+'q='+city+'&APPID=b35975e18dc93725acb092f7272cc6b8&units=metric'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# your code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Book name,price and stock availability as a pandas dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# This is the url you will scrape in this exercise. \n",
    "# It is a fictional bookstore created to be scraped. \n",
    "url = 'http://books.toscrape.com/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#your code"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
